{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [sample solution, trained for a few hours (not converged)]\n",
    "\n",
    "# This tutorial is will bring you through your first deep reinforcement learning model\n",
    "\n",
    "\n",
    "* Seaquest game as an example\n",
    "* Training a simple lasagne neural network for Q_learning objective\n",
    "\n",
    "\n",
    "## About OpenAI Gym\n",
    "\n",
    "* Its a recently published platform that basicly allows you to train agents in a wide variety of environments with near-identical interface.\n",
    "* This is twice as awesome since now we don't need to write a new wrapper for every game\n",
    "* Go check it out!\n",
    "  * Blog post - https://openai.com/blog/openai-gym-beta/\n",
    "  * Github - https://github.com/openai/gym\n",
    "\n",
    "\n",
    "## New to Lasagne and AgentNet?\n",
    "* We only require surface level knowledge of theano and lasagne, so you can just learn them as you go.\n",
    "* Alternatively, you can find Lasagne tutorials here:\n",
    " * Official mnist example: http://lasagne.readthedocs.io/en/latest/user/tutorial.html\n",
    " * From scratch: https://github.com/ddtm/dl-course/tree/master/Seminar4\n",
    " * From theano: https://github.com/craffel/Lasagne-tutorial/blob/master/examples/tutorial.ipynb\n",
    "* This is pretty much the basic tutorial for AgentNet, so it's okay not to know it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment setup\n",
    "* Here we basically just load the game and check that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS=\"floatX=float32\"\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "%env THEANO_FLAGS=\"floatX=float32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#global params.\n",
    "GAME = \"CartPole-v0\"\n",
    "\n",
    "#number of parallel agents and batch sequence length (frames)\n",
    "N_AGENTS = 1\n",
    "SEQ_LENGTH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-25 16:15:03,312] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02822146 -0.20307278 -0.02872299  0.30772524]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gym\n",
    "env = gym.make(GAME)\n",
    "obs = env.step(0)[0]\n",
    "action_names = np.array([\"left\",\"right\"]) #i guess so... i may be wrong\n",
    "state_size = len(obs)\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic agent setup\n",
    "Here we define a simple agent that maps game images into Qvalues using shallow neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import InputLayer,DenseLayer,batch_norm,dropout\n",
    "#image observation at current tick goes here, shape = (sample_i,x,y,color)\n",
    "observation_layer = InputLayer((None,state_size))\n",
    "\n",
    "dense0 = DenseLayer(observation_layer,100,\n",
    "                    name='dense',\n",
    "                    nonlinearity = lasagne.nonlinearities.tanh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a layer that predicts Qvalues\n",
    "qvalues_layer = DenseLayer(dense0,\n",
    "                   num_units = env.action_space.n,\n",
    "                   nonlinearity=lasagne.nonlinearities.linear,\n",
    "                   name=\"q-evaluator layer\")\n",
    "\n",
    "#To pick actions, we use an epsilon-greedy resolver (epsilon is a property)\n",
    "from agentnet.resolver import EpsilonGreedyResolver\n",
    "action_layer = EpsilonGreedyResolver(qvalues_layer,name=\"e-greedy action picker\")\n",
    "\n",
    "action_layer.epsilon.set_value(np.float32(0.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.target_network import TargetNetwork\n",
    "\n",
    "targetnet = TargetNetwork(qvalues_layer)\n",
    "old_qvalues_layer = targetnet.output_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally, agent\n",
    "We declare that this network is and MDP agent with such and such inputs, states and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.agent import Agent\n",
    "#all together\n",
    "agent = Agent(observation_layers=observation_layer,\n",
    "              policy_estimators=(qvalues_layer,old_qvalues_layer),\n",
    "              action_layers=action_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dense.W, dense.b, q-evaluator layer.W, q-evaluator layer.b]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params(action_layer,trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and manage a pool of atari sessions to play with\n",
    "\n",
    "* To make training more stable, we shall have an entire batch of game sessions each happening independent of others\n",
    "* Why several parallel agents help training: http://arxiv.org/pdf/1602.01783v1.pdf\n",
    "* Alternative approach: store more sessions: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-25 16:15:11,557] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "from agentnet.experiments.openai_gym.pool import EnvPool\n",
    "\n",
    "pool = EnvPool(agent,GAME, N_AGENTS,max_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['left' 'right' 'right' 'left' 'right' 'left' 'left']]\n",
      "[[ 1.  1.  1.  1.  1.  1.  0.]]\n",
      "CPU times: user 7 ms, sys: 0 ns, total: 7 ms\n",
      "Wall time: 5.69 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#interact for 7 ticks\n",
    "_,action_log,reward_log,_,_,_  = pool.interact(7)\n",
    "\n",
    "\n",
    "print(action_names[action_log][:2])\n",
    "print(reward_log[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load first sessions (this function calls interact and remembers sessions)\n",
    "pool.update(SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "* An agent has a method that produces symbolic environment interaction sessions\n",
    "* Such sessions are in sequences of observations, agent memory, actions, q-values,etc\n",
    "  * one has to pre-define maximum session length.\n",
    "\n",
    "* SessionPool also stores rewards (Q-learning objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get agent's Qvalues obtained via experience replay\n",
    "replay = pool.experience_replay.sample_session_batch(100,replace=True)\n",
    "\n",
    "_,_,_,_,(qvalues_seq,old_qvalues_seq) = agent.get_sessions(\n",
    "    replay,\n",
    "    session_length=SEQ_LENGTH,\n",
    "    optimize_experience_replay=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get reference Qvalues according to Qlearning algorithm\n",
    "from agentnet.learning import qlearning\n",
    "\n",
    "#crop rewards to [-1,+1] to avoid explosion.\n",
    "#import theano.tensor as T\n",
    "#rewards = T.maximum(-1,T.minimum(rewards,1))\n",
    "\n",
    "#loss for Qlearning = (Q(s,a) - (r+gamma*Q(s',a_max)))^2\n",
    "\n",
    "elwise_mse_loss = qlearning.get_elementwise_objective(qvalues_seq,\n",
    "                                                      replay.actions,\n",
    "                                                      replay.rewards,\n",
    "                                                      replay.is_alive,\n",
    "                                                      Qvalues_target=old_qvalues_seq,\n",
    "                                                      gamma_or_gammas=0.99,)\n",
    "\n",
    "#compute mean over \"alive\" fragments\n",
    "loss = elwise_mse_loss.sum() / replay.is_alive.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute weight updates\n",
    "updates = lasagne.updates.adadelta(loss,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compile train function\n",
    "import theano\n",
    "train_step = theano.function([],loss,updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-25 16:16:17,731] Making new env: CartPole-v0\n",
      "[2016-11-25 16:16:17,738] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2016-11-25 16:16:17,856] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/apanin/jheuristic/vime/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 138 timesteps with reward=138.0\n"
     ]
    }
   ],
   "source": [
    "untrained_reward = pool.evaluate(save_path=\"./records\",record_video=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "#video_path=\"./records/openaigym.video.0.7346.video000000.mp4\"\n",
    "\n",
    "#HTML(\"\"\"\n",
    "#<video width=\"640\" height=\"480\" controls>\n",
    "#  <source src=\"{}\" type=\"video/mp4\">\n",
    "#</video>\n",
    "#\"\"\".format(video_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bnn import bbpwrap, NormalApproximation,sample_output\n",
    "from lasagne.layers import EmbeddingLayer\n",
    "import theano.tensor as T\n",
    "@bbpwrap(NormalApproximation())\n",
    "class BayesDenseLayer(DenseLayer):pass\n",
    "@bbpwrap(NormalApproximation())\n",
    "class BayesEmbLayer(EmbeddingLayer):pass\n",
    "\n",
    "from curiosity import compile_vime_reward\n",
    "\n",
    "class BNN:\n",
    "    curiosity=0.1\n",
    "    target_rho = 1\n",
    "    \n",
    "    l_state = InputLayer((None,state_size),name='state var')\n",
    "    l_action = InputLayer((None,),input_var=T.ivector())\n",
    "\n",
    "    l_action_emb = BayesEmbLayer(l_action,env.action_space.n, 3)    \n",
    "    \n",
    "    l_concat = lasagne.layers.concat([l_action_emb,l_state])\n",
    "    \n",
    "    l_dense = BayesDenseLayer(l_concat,num_units=30,\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh)\n",
    "    \n",
    "    l_out = BayesDenseLayer(l_dense,num_units=state_size,\n",
    "                            nonlinearity=None)\n",
    "        \n",
    "    params = lasagne.layers.get_all_params(l_out,trainable=True)\n",
    "    ###training###\n",
    "    pred_states = lasagne.layers.get_output(l_out)\n",
    "    next_states = T.matrix(\"next states\")\n",
    "    mse = lasagne.objectives.squared_error(pred_states,next_states).mean()\n",
    "    \n",
    "    #replace logposterior with simple regularization on rho cuz we're lazy\n",
    "    reg = sum([lasagne.objectives.squared_error(rho,target_rho).mean() \n",
    "              for rho in lasagne.layers.get_all_params(l_out,rho=True)])\n",
    "    \n",
    "    loss = mse+ 0.01*reg\n",
    "    \n",
    "    updates = lasagne.updates.adam(loss,params)\n",
    "    \n",
    "    train_step = theano.function([l_state.input_var,l_action.input_var,next_states],\n",
    "                                 loss,updates=updates)\n",
    "    \n",
    "    ###sample random sessions from pool###\n",
    "    observations, = replay.observations\n",
    "    actions, = replay.actions\n",
    "    observations_flat = observations[:,:-1].reshape((-1,)+tuple(observations.shape[2:]))\n",
    "    actions_flat = actions[:,:-1].reshape((-1,))\n",
    "    next_observations_flat = observations[:,1:].reshape((-1,)+tuple(observations.shape[2:]))\n",
    "    sample_from_pool = theano.function([],[observations_flat,actions_flat,next_observations_flat])\n",
    "\n",
    "    \n",
    "    ###curiosity reward### aka KL(qnew,qold)\n",
    "    get_vime_reward_elwise = compile_vime_reward(l_out,l_state,l_action,params,n_samples=10)\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_vime_reward(observations,actions,rewards,is_alive,h0):\n",
    "        assert isinstance(observations,np.ndarray)\n",
    "        observations_flat = observations[:,:-1].reshape((-1,)+observations.shape[2:]).astype('float32')\n",
    "        actions_flat = actions[:,:-1].reshape((-1,)).astype('int32')\n",
    "        next_observations_flat = observations[:,1:].reshape((-1,)+observations.shape[2:]).astype('float32')\n",
    "\n",
    "        vime_rewards = BNN.get_vime_reward_elwise(observations_flat,actions_flat,next_observations_flat)\n",
    "        vime_rewards = np.concatenate([vime_rewards.reshape(rewards[:,:-1].shape),\n",
    "                                       np.zeros_like(rewards[:,-1:]),], axis=1)\n",
    "        surrogate_rewards = rewards + BNN.curiosity*vime_rewards\n",
    "        return (observations,actions,surrogate_rewards,is_alive,h0)\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#starting epoch\n",
    "epoch_counter = 1\n",
    "\n",
    "#full game rewards\n",
    "rewards = {epoch_counter:untrained_reward}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_games = 20\n",
    "action_layer.epsilon.set_value(0)\n",
    "rewards[epoch_counter] = pool.evaluate( record_video=False,n_games=n_games,verbose=False)\n",
    "print(\"Current score(mean over %i) = %.3f\"%(n_games,np.mean(rewards[epoch_counter])))\n",
    "action_layer.epsilon.set_value(np.float32(current_epsilon))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=10\tepsilon=0.491\treward/step=1.08267\n",
      "iter=20\tepsilon=0.482\treward/step=1.10625\n",
      "iter=30\tepsilon=0.474\treward/step=1.09475\n",
      "iter=40\tepsilon=0.465\treward/step=1.09015\n",
      "iter=50\tepsilon=0.457\treward/step=1.08826\n",
      "iter=60\tepsilon=0.449\treward/step=1.08557\n",
      "iter=70\tepsilon=0.441\treward/step=1.08216\n",
      "iter=80\tepsilon=0.433\treward/step=1.08321\n",
      "iter=90\tepsilon=0.426\treward/step=1.08629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-25 16:17:37,957] Making new env: CartPole-v0\n",
      "[2016-11-25 16:17:37,965] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2016-11-25 16:17:38,115] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/apanin/jheuristic/vime/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=100\tepsilon=0.418\treward/step=1.08570\n",
      "Current score(mean over 20) = 9.200\n",
      "iter=110\tepsilon=0.411\treward/step=1.08643\n",
      "iter=120\tepsilon=0.404\treward/step=1.08495\n",
      "iter=130\tepsilon=0.397\treward/step=1.08405\n",
      "iter=140\tepsilon=0.390\treward/step=1.08354\n",
      "iter=150\tepsilon=0.383\treward/step=1.08380\n",
      "iter=160\tepsilon=0.377\treward/step=1.08260\n",
      "iter=170\tepsilon=0.370\treward/step=1.08004\n",
      "iter=180\tepsilon=0.364\treward/step=1.07972\n",
      "iter=190\tepsilon=0.358\treward/step=1.07963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-25 16:17:57,862] Making new env: CartPole-v0\n",
      "[2016-11-25 16:17:57,868] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=200\tepsilon=0.352\treward/step=1.07931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-25 16:18:00,251] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/apanin/jheuristic/vime/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score(mean over 20) = 190.750\n",
      "iter=210\tepsilon=0.346\treward/step=1.07800\n",
      "iter=220\tepsilon=0.340\treward/step=1.07755\n",
      "iter=230\tepsilon=0.334\treward/step=1.07600\n",
      "iter=240\tepsilon=0.328\treward/step=1.07545\n",
      "iter=250\tepsilon=0.323\treward/step=1.07557\n",
      "iter=260\tepsilon=0.318\treward/step=1.07455\n",
      "iter=270\tepsilon=0.312\treward/step=1.07368\n",
      "iter=280\tepsilon=0.307\treward/step=1.07446\n",
      "iter=290\tepsilon=0.302\treward/step=1.07355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-25 16:18:20,714] Making new env: CartPole-v0\n",
      "[2016-11-25 16:18:20,721] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=300\tepsilon=0.297\treward/step=1.07315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-25 16:18:22,256] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/apanin/jheuristic/vime/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score(mean over 20) = 124.250\n",
      "iter=310\tepsilon=0.292\treward/step=1.07200\n",
      "iter=320\tepsilon=0.287\treward/step=1.07167\n",
      "iter=330\tepsilon=0.283\treward/step=1.07090\n",
      "iter=340\tepsilon=0.278\treward/step=1.06943\n",
      "iter=350\tepsilon=0.273\treward/step=1.06866\n",
      "iter=360\tepsilon=0.269\treward/step=1.06793\n",
      "iter=370\tepsilon=0.265\treward/step=1.06758\n",
      "iter=380\tepsilon=0.260\treward/step=1.06661\n",
      "iter=390\tepsilon=0.256\treward/step=1.06599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-25 16:18:42,593] Making new env: CartPole-v0\n",
      "[2016-11-25 16:18:42,602] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=400\tepsilon=0.252\treward/step=1.06519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-25 16:18:45,006] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/apanin/jheuristic/vime/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score(mean over 20) = 190.200\n",
      "iter=410\tepsilon=0.248\treward/step=1.06457\n",
      "iter=420\tepsilon=0.244\treward/step=1.06445\n",
      "iter=430\tepsilon=0.240\treward/step=1.06415\n",
      "iter=440\tepsilon=0.237\treward/step=1.06407\n",
      "iter=450\tepsilon=0.233\treward/step=1.06417\n",
      "iter=460\tepsilon=0.229\treward/step=1.06366\n",
      "iter=470\tepsilon=0.226\treward/step=1.06293\n",
      "iter=480\tepsilon=0.222\treward/step=1.06334\n",
      "iter=490\tepsilon=0.219\treward/step=1.06261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-25 16:19:05,495] Making new env: CartPole-v0\n",
      "[2016-11-25 16:19:05,501] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=500\tepsilon=0.216\treward/step=1.06227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-25 16:19:07,308] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/apanin/jheuristic/vime/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score(mean over 20) = 141.250\n",
      "iter=510\tepsilon=0.212\treward/step=1.06196\n",
      "iter=520\tepsilon=0.209\treward/step=1.06100\n",
      "iter=530\tepsilon=0.206\treward/step=1.06029\n",
      "iter=540\tepsilon=0.203\treward/step=1.06029\n",
      "iter=550\tepsilon=0.200\treward/step=1.06034\n",
      "iter=560\tepsilon=0.197\treward/step=1.05978\n",
      "iter=570\tepsilon=0.194\treward/step=1.05963\n",
      "iter=580\tepsilon=0.191\treward/step=1.05935\n",
      "iter=590\tepsilon=0.188\treward/step=1.05879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-25 16:19:27,679] Making new env: CartPole-v0\n",
      "[2016-11-25 16:19:27,685] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=600\tepsilon=0.186\treward/step=1.05825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-25 16:19:30,055] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/apanin/jheuristic/vime/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score(mean over 20) = 188.950\n",
      "iter=610\tepsilon=0.183\treward/step=1.05788\n",
      "iter=620\tepsilon=0.180\treward/step=1.05770\n",
      "iter=630\tepsilon=0.178\treward/step=1.05714\n",
      "iter=640\tepsilon=0.175\treward/step=1.05649\n",
      "iter=650\tepsilon=0.173\treward/step=1.05586\n",
      "iter=660\tepsilon=0.170\treward/step=1.05543\n",
      "iter=670\tepsilon=0.168\treward/step=1.05505\n",
      "iter=680\tepsilon=0.165\treward/step=1.05462\n",
      "iter=690\tepsilon=0.163\treward/step=1.05419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-25 16:19:50,541] Making new env: CartPole-v0\n",
      "[2016-11-25 16:19:50,548] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=700\tepsilon=0.161\treward/step=1.05381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-25 16:19:52,807] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/apanin/jheuristic/vime/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score(mean over 20) = 177.600\n",
      "iter=710\tepsilon=0.159\treward/step=1.05345\n",
      "iter=720\tepsilon=0.157\treward/step=1.05311\n",
      "iter=730\tepsilon=0.155\treward/step=1.05277\n",
      "iter=740\tepsilon=0.152\treward/step=1.05233\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#the loop may take eons to finish.\n",
    "#consider interrupting early.\n",
    "for i in range(10000):    \n",
    "    \n",
    "    \n",
    "    #train\n",
    "    pool.update(SEQ_LENGTH,append=True,preprocess=BNN.add_vime_reward)\n",
    "    \n",
    "    for i in range(10):\n",
    "        loss = train_step()\n",
    "        \n",
    "    BNN.train_step(*BNN.sample_from_pool())\n",
    "        \n",
    "    targetnet.load_weights(0.01)\n",
    "    \n",
    "    \n",
    "    ##update resolver's epsilon (chance of random action instead of optimal one)\n",
    "    current_epsilon = 0.05 + 0.45*np.exp(-epoch_counter/500.)\n",
    "    action_layer.epsilon.set_value(np.float32(current_epsilon))\n",
    "    \n",
    "    if epoch_counter%10==0:\n",
    "        #average reward per game tick in current experience replay pool\n",
    "        pool_mean_reward = pool.experience_replay.rewards.get_value().mean()\n",
    "        print(\"iter=%i\\tepsilon=%.3f\\treward/step=%.5f\"%(epoch_counter,\n",
    "                                                         current_epsilon,\n",
    "                                                         pool_mean_reward))\n",
    "        \n",
    "\n",
    "    ##record current learning progress and show learning curves\n",
    "    if epoch_counter%100 ==0:\n",
    "        n_games = 20\n",
    "        action_layer.epsilon.set_value(0)\n",
    "        rewards[epoch_counter] = pool.evaluate( record_video=False,n_games=n_games,verbose=False)\n",
    "        print(\"Current score(mean over %i) = %.3f\"%(n_games,np.mean(rewards[epoch_counter])))\n",
    "        action_layer.epsilon.set_value(np.float32(current_epsilon))\n",
    "    \n",
    "    \n",
    "    epoch_counter  +=1\n",
    "\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
